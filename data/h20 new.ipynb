{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpylab\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "\n",
    "def preprocess_data(targets, observed, estimated, test):\n",
    "    \"\"\"\n",
    "    Preprocess the data by resampling, merging with targets, and dropping unnecessary columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - targets: Target dataframe with 'time' and target values.\n",
    "    - observed: Dataframe with observed features.\n",
    "    - estimated: Dataframe with estimated features.\n",
    "    - test: Dataframe with test features.\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed dataframes ready for training and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the datetime columns are in datetime format\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    # Ensure data is sorted by date_forecast\n",
    "    targets = targets.sort_values(by='time')\n",
    "    observed = observed.sort_values(by='date_forecast')\n",
    "    estimated = estimated.sort_values(by='date_forecast')\n",
    "    test = test.sort_values(by='date_forecast')\n",
    "\n",
    "    # Identify boolean columns\n",
    "    boolean_features = [col for col in observed.columns if observed[col].dropna().isin([0.0, 1.0]).all()]\n",
    "\n",
    "    # Forward fill NaNs for boolean columns\n",
    "    for df in [observed, estimated, test]:\n",
    "        df[boolean_features] = df[boolean_features].fillna(method='ffill')\n",
    "\n",
    "    # Forward fill for time-series data (for non-boolean columns)\n",
    "    for df in [observed, estimated, test]:\n",
    "        df[df.columns.difference(boolean_features)] = df[df.columns.difference(boolean_features)].fillna(method='ffill')\n",
    "\n",
    "    \"\"\"  \n",
    "    # Forward fill for time-series data\n",
    "    observed.fillna(method='ffill', inplace=True)\n",
    "    estimated.fillna(method='ffill', inplace=True)\n",
    "    test.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Fill NaNs in boolean features with 0\n",
    "    boolean_features = [col for col in observed.columns if observed[col].dropna().isin([0.0, 1.0]).all()]\n",
    "    observed[boolean_features] = observed[boolean_features].fillna(method='ffill')\n",
    "    estimated[boolean_features] = estimated[boolean_features].fillna(method='ffill')\n",
    "    test[boolean_features] = test[boolean_features].fillna(method='ffill') \n",
    "    \"\"\"\n",
    "\n",
    "    # Resample observed, estimated, and test data to 1 hour using mean() as aggregator\n",
    "    # and drop rows where all columns are NaN\n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "\n",
    "    # Round boolean columns after resampling\n",
    "    for df in [observed_resampled, estimated_resampled, test_resampled]:\n",
    "        df[boolean_features] = df[boolean_features].round(0)\n",
    "\n",
    "    observed_resampled['estimated'] = 0\n",
    "    estimated_resampled['estimated'] = 1\n",
    "    test_resampled['estimated'] = 1\n",
    "    \n",
    "    # Merge the observed and estimated data\n",
    "    weather_data = pd.concat([observed_resampled, estimated_resampled])\n",
    "\n",
    "    # Merge with target values\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    # Time-Based Features (training data)\n",
    "    merged_data['hour'] = merged_data['date_forecast'].dt.hour\n",
    "    merged_data['sin_hour'] = np.sin(2 * np.pi * merged_data['hour'] / 24)\n",
    "    merged_data['cos_hour'] = np.cos(2 * np.pi * merged_data['hour'] / 24)\n",
    "    # merged_data['day_of_week'] = merged_data['date_forecast'].dt.dayofweek\n",
    "    merged_data['month'] = merged_data['date_forecast'].dt.month\n",
    "    merged_data['sin_month'] = np.sin(2 * np.pi * merged_data['month'] / 12)\n",
    "    merged_data['cos_month'] = np.cos(2 * np.pi * merged_data['month'] / 12)\n",
    "\n",
    "    # Time-Based Features (test data)\n",
    "    test_resampled['hour'] = test_resampled['date_forecast'].dt.hour\n",
    "    test_resampled['sin_hour'] = np.sin(2 * np.pi * test_resampled['hour'] / 24)\n",
    "    test_resampled['cos_hour'] = np.cos(2 * np.pi * test_resampled['hour'] / 24)\n",
    "    # test_resampled['day_of_week'] = test_resampled['date_forecast'].dt.dayofweek\n",
    "    test_resampled['month'] = test_resampled['date_forecast'].dt.month\n",
    "    test_resampled['sin_month'] = np.sin(2 * np.pi * test_resampled['month'] / 12)\n",
    "    test_resampled['cos_month'] = np.cos(2 * np.pi * test_resampled['month'] / 12)\n",
    "\n",
    "\n",
    "    # Drop non-feature columns\n",
    "    merged_data = merged_data.drop(columns=['time', 'date_forecast', 'pv_measurement', 'snow_density:kgm3'])\n",
    "    test_resampled = test_resampled.drop(columns=['date_forecast', 'snow_density:kgm3'])\n",
    "\n",
    "    # fixing ceiling_height NaN value\n",
    "    merged_data['ceiling_height_agl:m'].fillna(0, inplace=True)\n",
    "    test_resampled['ceiling_height_agl:m'].fillna(0, inplace=True)\n",
    "  \n",
    "    \n",
    "    return merged_data, test_resampled\n",
    "\n",
    "locations = ['A', 'B', 'C']\n",
    "location_mapping = {'A': 1, 'B': 2, 'C': 3}\n",
    "all_predictions = []\n",
    "all_predictions_rf = []\n",
    "\n",
    "\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # save as csv for analysis\n",
    "    \"\"\" train.to_csv(f'{loc}_csv/train_targets.csv')\n",
    "    X_train_estimated.to_csv(f'{loc}_csv/X_train_estimated.csv')\n",
    "    X_train_observed.to_csv(f'{loc}_csv/X_train_observed.csv')\n",
    "    X_test_estimated.to_csv(f'{loc}_csv/X_test_estimated.csv') \"\"\"\n",
    "\n",
    "   # Preprocess data\n",
    "    X_train, X_test = preprocess_data(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    \n",
    "    X_train['location'] = location_mapping[loc]\n",
    "    X_test['location'] = location_mapping[loc]\n",
    "\n",
    "    X_train.to_csv('X_train.csv')\n",
    "    X_test.to_csv('X_test.csv')\n",
    "\n",
    "    y = train['pv_measurement'].values\n",
    "\n",
    "\n",
    "    # Ensure X and y have the same length\n",
    "    min_length = min(len(X_train), len(y))\n",
    "    X_train, y_train = X_train.iloc[:min_length], y[:min_length]\n",
    "\n",
    "    X_train_data, X_eval_data, y_train_data, y_eval_data = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create catboost Pool objects\n",
    "    train_pool = Pool(data=X_train_data, label=y_train_data, cat_features=['location', 'estimated'])\n",
    "    eval_pool = Pool(data=X_eval_data, label=y_eval_data, cat_features=['location', 'estimated'])\n",
    "\n",
    "    # Initialize and Train model\n",
    "    #categorical_features = ['dew_or_rime:idx', 'elevation:m', 'is_day:idx', 'is_in_shadow:idx', 'snow_drift:idx', 'wind_speed_w_1000hPa:ms']\n",
    "\n",
    "    model = CatBoostRegressor(depth=9, iterations=1000, loss_function='MAE')\n",
    "    model.fit(train_pool, use_best_model=True, eval_set=eval_pool)\n",
    "\n",
    "    # Make predictions using X_test_estimated data\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Store the predictions in all_predictions list\n",
    "    all_predictions.append(predictions)\n",
    "\n",
    "    \"\"\" \n",
    "    # Initialize and Train RandomForest model\n",
    "    model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using X_test data\n",
    "    predictions_rf = model_rf.predict(X_test)\n",
    "    \n",
    "    # Store the RandomForest predictions in all_predictions_rf list\n",
    "    all_predictions_rf.append(predictions_rf) \n",
    "    \n",
    "    final_predictions_rf = np.concatenate(all_predictions_rf)\n",
    "    \n",
    "    average_predictions = (np.array(final_predictions) + np.array(final_predictions_rf)) / 2.0\n",
    "    \"\"\"\n",
    "\n",
    "# Concatenate all predictions\n",
    "final_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df['prediction'] = df['prediction'].apply(lambda x: max(0, x))\n",
    "df.to_csv('final_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
